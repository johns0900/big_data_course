test.csv file content:
id, name
1,aaa
2,bbb
3,ccc
4,ddd



creat a testing csv file

-- the following command returns RDD
txF = sc.textFile("/home/student/Documents/test.csv")


-- check the type of the object
type(txF)

DataFramework provide better performance, because of better schema, which has the whole structure

Spark optimizer  (tanster) can work very well structured data


--- read data file directlry to DataFrame, without using map & ToDF() functions
txDF = spark.read.csv("/home/student/Documents/test.csv", header=True)
txDF.printSchema() ---- all columns are string data type

-- inferSchema=True :: decide the data type automatically by the real data
txDF = spark.read.csv("/home/student/Documents/test.csv", header=True, inferSchema=True)

-- if read json file, the returned result is DataFrame
-- better create schema object, by sparksql.type, to define structtype
reference lesson 1 , task 4
schema = StructType([StructField("account_id", IntegerType(), True), 
example:
-------
Task 4 
 
Multiple ways of creating dataframe using sqlContext Notebook: https://databricks-prodcloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/10921766855316 50/3530701261005487/6776489139542437/latest.html 
 
txF = sc.textFile("<File Path>") tx1 = txF.map(lambda x: x.split(",")) 
 
1. Ambiguous column names 
 
 sqlContext.createDataFrame(tx1) 
 
2. Specified column names -  Approach 1 
 
sqlContext.createDataFrame(tx1, ['account_id', 'balance']) 
 
3. Specified column names - Approach 2 
 
from pyspark.sql import Row account = Row(‘account_id’, ‘balance’) tx2 = tx1.map(lambda x: account(*x)) sqlContext.createDataFrame(tx2) 
 
4. Applying schema more sophisticated way 
 
from pyspark.sql.types import * tx2 = tx1.map(lambda x: (int(x[0]),int(x[1]))) schema = StructType([StructField("account_id", IntegerType(), True), StructField("balance", IntegerType(), True)]) sqlContext.createDataFrame(tx2, schema) 
 
 NOTE: try creating df directly from tx1 and see the result 
 
5. We can also create dataframe from native python pandas dataframe but that has been kept as an exercise for learners 
 
 -------

Machine Learning:

Supervised learning:
  regression: 
	liner regression
  classification:
	logistic
	KNN
	Navie Bayers
	Random Forest



Unsupervised ML
-- Topic Modelling

------------------------
Liner regression
	size of house x, price y
xi ---> h (hypothesis) --> yi

Grident Descent algoriation

alpha: learning rate,    normally 0.001 ??

multi nomial or poly nomial algorithms??



vector:
sparse vector  [0,0,0,0,0,1,0,0,0,5], dense vector [1,2,3,5,6,7,0, 9]
[8, ((4,1), (8, 5))]


a feature vector needs to be created for ML



----------- exercise ------------------
pip install --upgrade pip
sudo python -m pip install numpy

df.columns
feature_columns = df.columns[1:]


--from pyspark.sql import SparkSession
--spark = SparkSession.builder.getOrCreate()

data = spark.read.csv("/home/student/Documents/dataLesson2/mlinput.csv", inferSchema=True)
data.printSchema()
feature_columns = data.columns[1:]

# Need to install numpy if haven't. 
# Command: pip install numpy
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=feature_columns,outputCol="features")
transformed_data = assembler.transform(data)

train, test = transformed_data.randomSplit([0.8, 0.2])

from pyspark.ml.regression import LinearRegression
linearregression = LinearRegression(featuresCol="features", labelCol="_c0")
model = linearregression.fit(train)

predictions = model.transform(test)

predictions.show()

-------------------------------------

spark two ML APIs
MLlib  --- RDD, not be supported well
ml   ---DF, better performance



correlatioin matrix, to check different factors relataionship
example:
	salary	experience	health
salary	1	0.8		-0.56
experience 0.8		0	0
health    -0.56		0	1

pearson
spearman


-----------------------------------------------

Independence hypothesis 


transformer adds one new column to existing dataframe


------------------ task 2 -----------------------------------------

https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/chi-square/


Task 2: Chi-Square hypothesis test 
 
Objective: Based on employee sample data check if salary depends on role. 
 
#Read data 
df = spark.read.csv("/home/student/Documents/dataLesson2/developers_survey_training.csv", header='true') 
 
#Replace IsDeveloper value with integer 1 or 0 
df.createOrReplaceTempView("inputData") 
df1 = spark.sql("SELECT CASE IsDeveloper WHEN 'Yes' THEN 1 ELSE 0 END AS IsDeveloper, CASE WHEN YearsOfExp<=2 THEN 1 WHEN YearsOfExp>2 AND YearsOfExp<=5 THEN 2 ELSE 3 END AS YearsOfExp, CASE WHEN Salary<=50000 THEN 1 WHEN  Salary>50000 AND Salary<100000 THEN 2 ELSE 3 END AS Salary FROM inputData "); 
 
#Create feature vector 
from pyspark.ml.feature import VectorAssembler 
assembler = VectorAssembler(inputCols=["IsDeveloper","YearsOfExp"], outputCol="features") 
combined = assembler.transform(df1) 
vector_df = combined.select(combined.Salary, combined.features) 
 
#Find the chi-square stats 
from pyspark.ml.stat import ChiSquareTest 
chiResult = ChiSquareTest.test(vector_df, "features", "Salary") 
 
#Display the stats 
chiResult.head().pValues 
chiResult.head().degreesOfFreedom 
 

---------------- task 3 ----------------------------------------------------
Task 3: Skeleton of an ML program: Transformer, Estimator, Parameters 
 Objective:​ Understand the building block of any ML application with the example of predicting the role of an IT professional is developer by looking at his experience and annual salary 
 
#Read data 
df = spark.read.csv("/home/student/Documents/dataLesson2/developers_survey_training.csv", header='true') 
 
#Replace IsDeveloper value with integer 1 or 0 
df.createOrReplaceTempView("inputData") 
df1 = spark.sql("SELECT CASE IsDeveloper WHEN 'Yes' THEN 1 ELSE 0 END AS label, CAST(YearsOfExp AS FLOAT) AS YearsOfExp, CAST(Salary AS FLOAT) AS Salary FROM inputData "); 
 
#Create feature vector 
from pyspark.ml.feature import VectorAssembler 
assembler = VectorAssembler(inputCols=["Salary","YearsOfExp"], outputCol="features") 
combined = assembler.transform(df1) 
vector_df = combined.select(combined.label, combined.features) 


#​Estimator: ​Create an instance LogisticRegression which is an estimator  
from pyspark.ml.classification import LogisticRegression 
lr_estimator = LogisticRegression(maxIter=10) 
print str(LogisticRegression().explainParams()) 
 
#Train the model 
model = lr_estimator.fit(vector_df) 

#​Parameters​: Check the parameters used to train the model 
params = model.extractParamMap() 
 
#Pass parameters explicitly while training the model 
params = {lr_estimator.maxIter:15} 
model = lr_estimator.fit(vector_df, params) 
 
#​Transformer​: test the model. Transform method will return a dataframe with predictions 
prediction = model.transform(vector_df) 
 
#​Save​ the model on disc  
model.save("/home/student/Documents/dataLesson2/output/predict_emp_role") 
 

#​Load​ a trained model from disc to memory 
from pyspark.ml.classification import LogisticRegressionModel 
mymodel = LogisticRegressionModel.load("/home/student/Documents/dataLesson2/output/predict_emp_role") 
prediction = mymodel.transform(vector_df) 
 
#QUESTION: How do we predict in actual production environment? 
??


---------------------- task 4 ----------------------------------------------------

Task 4: Classification Algorithm: Text classification without pipeline
Objective: Classify hotel review text as positive or negative using LogisticRegression model

#Prepared input dataset
training = spark.createDataFrame([("We had a perfectly pleasant stay here in December.", 1),("Stayed at this hotel again and it was as good as last year. Great service, perfect location and very clean.", 1),("Very negative experience when trying to get a refund from my travelocity booking that was cancelled due to a weather delay .", 0), ("Staff is very green young, restaurant is nice but forget it worst service ever. breakfast, burned toasts, cold eggs waited forever to be seated and for the food. ", 0)], ["review", "label"])


#Tokenize sentences
from pyspark.ml.feature import Tokenizer
tokenizer = Tokenizer(inputCol="review", outputCol="words")

#Generate hash for each word
from pyspark.ml.feature import HashingTF
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")

#Create LogisticRegression instance
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(maxIter=10)

#Train the model
tkns = tokenizer.transform(training)
htf = hashingTF.transform(tkns)
model = lr.fit(htf)

#Predict
-- cannot copy this lines
test = spark.createDataFrame([("Very negative experience. Not impressed. I won't be staying here again and I recommend you don't either.",), ("Great service, perfect location and very clean.",), ("The staff were all excellent. Super pleasant and courteous.",), ("worst service ever. waited forever to be seated and for the food.",)], ["review"])

test_tkns = tokenizer.transform(test)
test_htf = hashingTF.transform(test_tkns)
prediction = model.transform(test_htf)

---------------------- task 5 ----------------------------------------------------
Task 5: Classification Algorithm: Text classification with pipeline
Objective: Demonstrate how pipeline helps to compac the code

#Prepared input dataset
training = spark.createDataFrame([
("We had a perfectly pleasant stay here in December.", 1),
("Stayed at this hotel again and it was as good as last year. Great service, perfect location
and very clean.", 1),
("Very negative experience when trying to get a refund from my travelocity booking that was
cancelled due to a weather delay .", 0),
("Staff is very green young, restaurant is nice but forget it worst service ever. breakfast,
burned toasts, cold eggs waited forever to be seated and for the food. ", 0)
], ["review", "label"])

#Tokenize sentences
from pyspark.ml.feature import Tokenizer
tokenizer = Tokenizer(inputCol="review", outputCol="words")


#Generate hash for each word
from pyspark.ml.feature import HashingTF
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")

#Create LogisticRegression instance
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(maxIter=10)

#Create ML pipeline
from pyspark.ml import Pipeline
lr_pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

#Train the model
model = lr_pipeline.fit(training)


#Predict
test = spark.createDataFrame([("Very negative experience. Not impressed. I won't be staying here again and I recommend you don't either.",), ("Great service, perfect location and very clean.",), ("The staff were all excellent. Super pleasant and courteous.",), ("worst service ever. waited forever to be seated and for the food.",)], ["review"])

prediction = model.transform(test)


---------------------- task 6 ----------------------------------------------------

Task 6: K-means
Objective: K-means clustering an instance of Unsupervised ML

#Read data
df = spark.read.csv("/home/student/Documents/dataLesson2/Task6/Wine.csv", header='true')

#Replace IsDeveloper value with integer 1 or 0
df1 = df.select(df.Alcohol.cast("float"), df.Malic_Acid.cast("float"), df.Ash.cast("float"), df.Ash_Alcanity.cast("float"), df.Magnesium.cast("float"), df.Total_Phenols.cast("float"),df.Flavanoids.cast("float"), df.Nonflavanoid_Phenols.cast("float"),df.Proanthocyanins.cast("float"), df.Color_Intensity.cast("float"), df.Hue.cast("float"),df.OD280.cast("float"), df.Proline.cast("float"))

#Create feature vector
from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=["Alcohol", "Malic_Acid", "Ash", "Ash_Alcanity","Magnesium", "Total_Phenols", "Flavanoids", "Nonflavanoid_Phenols", "Proanthocyanins","Color_Intensity", "Hue", "OD280", "Proline"], outputCol="features")
combined = assembler.transform(df1)
vector_df = combined.select(combined.features)

#Let the algorithm figure out different clusters
from pyspark.ml.clustering import KMeans
kmeans = KMeans().setK(3)
model = kmeans.fit(vector_df)

#Predict
predict = model.transform(vector_df)
predict.show()































help(spark.read.csv)











